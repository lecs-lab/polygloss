[config]

mode = pretrain
pretrained_model = google/t5gemma-2-270m-270m
model_type = seq2seq

# Dataset
dataset_key = lecslab/polygloss-corpus
task_format = interleaved

# Training
max_epochs = 75
learning_rate = 5e-5
batch_size = 18 
gradient_accumulation_steps = 3 
grad_norm = 1
