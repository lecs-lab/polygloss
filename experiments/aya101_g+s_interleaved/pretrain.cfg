[config]

mode = lora
pretrained_model = CohereLabs/aya-101
model_type = seq2seq

# Dataset
dataset_key = lecslab/polygloss-corpus
task_format = interleaved

# Training
max_epochs = 15
learning_rate = 5e-4
batch_size = 64
grad_norm = 1
optimizer = adamw

target_modules = ["q_proj", "v_proj"]
lora_rank = 16
