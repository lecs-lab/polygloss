[config]

mode = lora
pretrained_model = CohereLabs/aya-expanse-8b
model_type = decoder
assistant_response_token = <|CHATBOT_TOKEN|>
quantize = True

# Dataset
dataset_key = lecslab/polygloss-corpus
task_format = interleaved

# Training
max_epochs = 15
learning_rate = 2e-5
batch_size = 12
grad_norm = 1
optimizer = adamw

target_modules = ["q_proj", "v_proj"]
lora_rank = 8
lora_alpha = 16
