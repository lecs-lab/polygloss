[config]

mode = lora
pretrained_model = CohereLabs/aya-expanse-8b
model_type = decoder
quantize = True

# Dataset
dataset_key = lecslab/polygloss-corpus
task_format = interleaved

# Training
max_epochs = 10
learning_rate = 5e-4
batch_size = 12 
grad_norm = 1
optimizer = adamw

target_modules = ["q_proj", "v_proj"]
lora_rank = 8
