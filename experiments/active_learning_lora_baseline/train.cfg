[config]

mode = lora
pretrained_model = google/byt5-base

# Dataset
dataset_key = stupidfishlady/linggym-vamale
task_format = interleaved
glottocode = vama1243

# Training
weight_decay=0.01
max_epochs = 25
learning_rate = 2e-4
batch_size = 32
grad_norm = 1
optimizer = adamw
