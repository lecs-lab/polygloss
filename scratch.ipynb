{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning: double the outputs. Assuming segmentation and gloss\n",
      "✅ Merged CSV saved to: /projects/enri8153/polygloss/experiments/polygloss_peft/nort2938/predictions_with_gt.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def parse_test_input(input_path):\n",
    "    \"\"\"\n",
    "    Parse the test input file and return a list of (transcription, translation) tuples.\n",
    "    \"\"\"\n",
    "    entries = []\n",
    "    transcription = None\n",
    "    translation = None\n",
    "\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"\\\\t\"):\n",
    "                transcription = line[2:].strip()\n",
    "            elif line.startswith(\"\\\\l\"):\n",
    "                translation = line[2:].strip()\n",
    "                if transcription:\n",
    "                    entries.append((transcription, translation))\n",
    "                    transcription, translation = None, None\n",
    "    return entries\n",
    "\n",
    "\n",
    "def merge_with_outputs(input_txt, output_csv, merged_csv):\n",
    "    \"\"\"\n",
    "    Merge parsed transcriptions/translations with the output CSV using pandas.\n",
    "    \"\"\"\n",
    "    # Parse the transcription/translation pairs\n",
    "    pairs = parse_test_input(input_txt)\n",
    "    df_pairs = pd.DataFrame(pairs, columns=[\"transcription\", \"translation\"])\n",
    "\n",
    "    # Read the output CSV\n",
    "    df_out = pd.read_csv(output_csv)\n",
    "\n",
    "    # Align and merge (assumes same order and number of rows)\n",
    "    if len(df_out) != len(df_pairs):\n",
    "        if len(df_out) == 2 * len(df_pairs):\n",
    "            print(f\"⚠️ Warning: double the outputs. Assuming segmentation and gloss\")\n",
    "            df_pairs = df_pairs.loc[np.repeat(df_pairs.index, 2)].reset_index(drop=True)\n",
    "        else:\n",
    "            print(f\"⚠️ Warning: Mismatch in rows — output={len(df_out)}, inputs={len(df_pairs)}. Will align by index.\")\n",
    "    df_merged = pd.concat([df_out.reset_index(drop=True), df_pairs.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    # Save merged CSV\n",
    "    df_merged.to_csv(merged_csv, index=False, encoding=\"utf-8\")\n",
    "    print(f\"✅ Merged CSV saved to: {merged_csv}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "input_txt = \"/projects/enri8153/polygloss/data/nort2938/nort2938-test.txt\"      # input file with \\t and \\l lines\n",
    "output_csv = \"/projects/enri8153/polygloss/experiments/polygloss_peft/nort2938/predictions.csv\"    # file with predicted, reference, etc.\n",
    "merged_csv = \"/projects/enri8153/polygloss/experiments/polygloss_peft/nort2938/predictions_with_gt.csv\"  # output path\n",
    "\n",
    "merge_with_outputs(input_txt, output_csv, merged_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "path = \"/projects/enri8153/polygloss/wandb/run-20250918_163458-z71wr0x6/files/media/table/predictions_14847_bc257628e1e97d66196c.table.json\"\n",
    "# Load JSON\n",
    "with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Convert to DataFrame and save as CSV\n",
    "df = pd.DataFrame(data[\"data\"], columns=data[\"columns\"])\n",
    "df.to_csv(\"output.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[93.31455677438483, 87.31800766283524, 81.686230964467, 76.61515699490383]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'word_level': {'average_accuracy': 0.8062821967573969,\n",
       "  'accuracy': 0.7941986025831039},\n",
       " 'morpheme_accuracy': {'average_accuracy': 0.7880896507878158,\n",
       "  'accuracy': 0.7544560872852976},\n",
       " 'classes': {'stem': {'prec': 0.7506444808999297,\n",
       "   'rec': 0.7436730903180868,\n",
       "   'f1': 0.7471425239094939},\n",
       "  'gram': {'prec': 0.8094978165938864,\n",
       "   'rec': 0.8032715848770448,\n",
       "   'f1': 0.8063726822902505}},\n",
       " 'bleu': 84.21675369266296}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from glossing import eval\n",
    "df = pd.read_csv('/projects/enri8153/polygloss/experiments/igt_unsegmented_no_pretrain/dido1241/None/predictions.csv')\n",
    "eval.evaluate_glosses(df.predicted.tolist(), df.reference.tolist()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/projects/enri8153/software/anaconda/envs/polygloss/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,105,920 || all params: 582,759,168 || trainable%: 0.1898\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from peft import LoraConfig, TaskType\n",
    "from peft import get_peft_model\n",
    "\n",
    "peft_config = LoraConfig(task_type=TaskType.SEQ_2_SEQ_LM, inference_mode=False, r=8, lora_alpha=32, lora_dropout=0.1)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lecslab/polygloss_byt5_2025_09_18\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"lecslab/polygloss_byt5_2025_09_18\")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deduplicated CSV written to /projects/enri8153/polygloss/tsez_byt5_test.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "def deduplicate_csv(input_file, output_file, key_columns=None):\n",
    "    \"\"\"\n",
    "    Deduplicate rows in a CSV file.\n",
    "\n",
    "    :param input_file: Path to input CSV file\n",
    "    :param output_file: Path to save deduplicated CSV file\n",
    "    :param key_columns: List of column names or indices to use as deduplication keys.\n",
    "                        If None, the entire row is used.\n",
    "    \"\"\"\n",
    "    seen = set()\n",
    "    deduped_rows = []\n",
    "\n",
    "    with open(input_file, mode=\"r\", newline=\"\", encoding=\"utf-8\") as infile:\n",
    "        reader = csv.DictReader(infile)\n",
    "        fieldnames = reader.fieldnames\n",
    "\n",
    "        for row in reader:\n",
    "            # Create a unique identifier based on selected key columns or the whole row\n",
    "            if key_columns:\n",
    "                key = tuple(row[col] for col in key_columns)\n",
    "            else:\n",
    "                key = tuple(row.values())\n",
    "\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                deduped_rows.append(row)\n",
    "\n",
    "    # Write deduplicated rows to output\n",
    "    with open(output_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as outfile:\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(deduped_rows)\n",
    "\n",
    "    print(f\"Deduplicated CSV written to {output_file}\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    deduplicate_csv(\"/projects/enri8153/polygloss/tsez_byt5_test.csv\", \"/projects/enri8153/polygloss/tsez_byt5_test.csv\")  # deduplicate based on full row\n",
    "    # deduplicate_csv(\"input.csv\", \"output.csv\", key_columns=[\"id\", \"email\"])  # deduplicate based on specific columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "PytorchStreamReader failed reading zip archive: failed finding central directory",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m ckpt_path = \u001b[33m\"\u001b[39m\u001b[33m/scratch/alpine/enri8153/polygloss-peft-dido1241/chunk_0_10/egxgqorw.checkpoint.pt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# 2️⃣ Load checkpoint from state dict?\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m lora_model.load_state_dict(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/projects/enri8153/software/anaconda/envs/polygloss/lib/python3.13/site-packages/torch/serialization.py:1432\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[39m\n\u001b[32m   1430\u001b[39m orig_position = opened_file.tell()\n\u001b[32m   1431\u001b[39m overall_storage = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1432\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_zipfile_reader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[32m   1433\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_torchscript_zip(opened_zipfile):\n\u001b[32m   1434\u001b[39m         warnings.warn(\n\u001b[32m   1435\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtorch.load\u001b[39m\u001b[33m'\u001b[39m\u001b[33m received a zip file that looks like a TorchScript archive\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1436\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m dispatching to \u001b[39m\u001b[33m'\u001b[39m\u001b[33mtorch.jit.load\u001b[39m\u001b[33m'\u001b[39m\u001b[33m (call \u001b[39m\u001b[33m'\u001b[39m\u001b[33mtorch.jit.load\u001b[39m\u001b[33m'\u001b[39m\u001b[33m directly to\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1437\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m silence this warning)\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1438\u001b[39m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[32m   1439\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/projects/enri8153/software/anaconda/envs/polygloss/lib/python3.13/site-packages/torch/serialization.py:763\u001b[39m, in \u001b[36m_open_zipfile_reader.__init__\u001b[39m\u001b[34m(self, name_or_buffer)\u001b[39m\n\u001b[32m    762\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name_or_buffer) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m763\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPyTorchFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[31mRuntimeError\u001b[39m: PytorchStreamReader failed reading zip archive: failed finding central directory"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from peft import PeftModel, LoraConfig, TaskType, get_peft_model\n",
    "import torch\n",
    "\n",
    "base_model_name = \"lecslab/polygloss_byt5_2025_09_18\"\n",
    "adapter_dir = \"/scratch/alpine/enri8153/polygloss-peft-dido1241/chunk_0_10/dido1241/10\"\n",
    "lora_config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "            r=8,\n",
    "            lora_alpha=32,\n",
    "            lora_dropout=0.1\n",
    "        )\n",
    "# 1️⃣ Load the base model\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_name)\n",
    "\n",
    "# 2️⃣ Load the PEFT adapter\n",
    "lora_model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "\n",
    "ckpt_path = \"/scratch/alpine/enri8153/polygloss-peft-dido1241/chunk_0_10/egxgqorw.checkpoint.pt\"\n",
    "# 2️⃣ Load checkpoint from state dict?\n",
    "lora_model.load_state_dict(torch.load(ckpt_path))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the base model\n",
    "model = AutoModel.from_pretrained('BAAI/bge-multilingual-gemma2')\n",
    "model.load_state_dict(torch.load('base_model.pt'))\n",
    "\n",
    "# Load the adapter layers (PEFT part)\n",
    "lora_model = get_peft_model(model, lora_config)\n",
    "lora_model.load_state_dict(torch.load('adapter_model.pt'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glossy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
